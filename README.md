# ToÃ¡n á»¨ng Dá»¥ng trong TrÃ­ Tuá»‡ NhÃ¢n Táº¡o

## Giá»›i thiá»‡u
ToÃ¡n há»c lÃ  ná»n táº£ng cá»‘t lÃµi cá»§a TrÃ­ tuá»‡ nhÃ¢n táº¡o (AI) vÃ  Machine Learning (ML). TÃ i liá»‡u nÃ y cung cáº¥p tá»•ng quan chi tiáº¿t vá» cÃ¡c lÄ©nh vá»±c toÃ¡n há»c quan trá»ng cáº§n thiáº¿t Ä‘á»ƒ hiá»ƒu vÃ  phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng AI.

---

## ğŸ“š Má»¥c lá»¥c
1. [Äáº¡i sá»‘ tuyáº¿n tÃ­nh (Linear Algebra)](#1-Ä‘áº¡i-sá»‘-tuyáº¿n-tÃ­nh)
2. [Giáº£i tÃ­ch (Calculus)](#2-giáº£i-tÃ­ch)
3. [XÃ¡c suáº¥t vÃ  Thá»‘ng kÃª (Probability & Statistics)](#3-xÃ¡c-suáº¥t-vÃ -thá»‘ng-kÃª)
4. [Tá»‘i Æ°u hÃ³a (Optimization)](#4-tá»‘i-Æ°u-hÃ³a)
5. [LÃ½ thuyáº¿t thÃ´ng tin (Information Theory)](#5-lÃ½-thuyáº¿t-thÃ´ng-tin)
6. [Lá»™ trÃ¬nh há»c táº­p](#6-lá»™-trÃ¬nh-há»c-táº­p)
7. [TÃ i nguyÃªn há»c táº­p](#7-tÃ i-nguyÃªn-há»c-táº­p)

---

## 1. Äáº¡i sá»‘ tuyáº¿n tÃ­nh

### Táº§m quan trá»ng
Äáº¡i sá»‘ tuyáº¿n tÃ­nh lÃ  cÆ¡ sá»Ÿ cá»§a háº§u háº¿t cÃ¡c thuáº­t toÃ¡n ML/AI, tá»« xá»­ lÃ½ dá»¯ liá»‡u Ä‘áº¿n deep learning.

### CÃ¡c khÃ¡i niá»‡m chÃ­nh

#### 1.1 Vector vÃ  Ma tráº­n
- **Vector**: Äáº¡i diá»‡n cho Ä‘iá»ƒm dá»¯ liá»‡u trong khÃ´ng gian n chiá»u
  ```
  v = [xâ‚, xâ‚‚, ..., xâ‚™]
  ```
- **Ma tráº­n**: LÆ°u trá»¯ dá»¯ liá»‡u, trá»ng sá»‘ trong neural networks
  ```
  A = [aâ‚â‚  aâ‚â‚‚  ...  aâ‚â‚™]
      [aâ‚‚â‚  aâ‚‚â‚‚  ...  aâ‚‚â‚™]
      [...  ...  ...  ...]
      [aâ‚˜â‚  aâ‚˜â‚‚  ...  aâ‚˜â‚™]
  ```

**VÃ­ dá»¥ Python**:
```python
import numpy as np

# Táº¡o vector
v = np.array([1, 2, 3, 4])
print(f"Vector: {v}")
print(f"Chiá»u: {v.shape}")  # (4,)

# Táº¡o ma tráº­n
A = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]])
print(f"Ma tráº­n:\n{A}")
print(f"KÃ­ch thÆ°á»›c: {A.shape}")  # (3, 3)

# Vector row vÃ  column
row_vector = np.array([[1, 2, 3]])  # Shape: (1, 3)
col_vector = np.array([[1], [2], [3]])  # Shape: (3, 1)
```

#### 1.2 CÃ¡c phÃ©p toÃ¡n cÆ¡ báº£n

**1. Cá»™ng vector/ma tráº­n**:
```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = A + B  # [[6, 8], [10, 12]]
```

**2. NhÃ¢n vÃ´ hÆ°á»›ng (Scalar multiplication)**:
```python
A = np.array([[1, 2], [3, 4]])
scaled = 2 * A  # [[2, 4], [6, 8]]
```

**3. NhÃ¢n ma tráº­n**:
```python
# Matrix multiplication: (mÃ—n) Ã— (nÃ—p) = (mÃ—p)
A = np.array([[1, 2], [3, 4]])  # 2Ã—2
B = np.array([[5, 6], [7, 8]])  # 2Ã—2
C = np.dot(A, B)  # hoáº·c A @ B
# C = [[19, 22], [43, 50]]

# Trong Neural Networks
X = np.random.randn(100, 784)  # 100 samples, 784 features
W = np.random.randn(784, 128)  # Weights
b = np.random.randn(128)       # Bias
Y = X @ W + b  # Output: (100, 128)
```

**4. Chuyá»ƒn vá»‹ (Transpose)**:
```python
A = np.array([[1, 2, 3],
              [4, 5, 6]])
A_T = A.T  # [[1, 4],
           #  [2, 5],
           #  [3, 6]]
```

**5. TÃ­ch vÃ´ hÆ°á»›ng (Dot product)**:
```python
# Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 vectors
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

similarity = np.dot(v1, v2)  # 1*4 + 2*5 + 3*6 = 32

# Cosine similarity
cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
```

#### 1.3 Eigenvalues vÃ  Eigenvectors

**Äá»‹nh nghÄ©a**:
```
Av = Î»v
```
Trong Ä‘Ã³:
- A: Ma tráº­n vuÃ´ng (nÃ—n)
- v: Eigenvector (khÃ´ng thay Ä‘á»•i hÆ°á»›ng khi nhÃ¢n vá»›i A)
- Î»: Eigenvalue (há»‡ sá»‘ scale)

**Ã nghÄ©a**:
- Eigenvector: HÆ°á»›ng mÃ  ma tráº­n chá»‰ scale, khÃ´ng xoay
- Eigenvalue: Äá»™ lá»›n cá»§a scaling

**Code Python**:
```python
import numpy as np

# Ma tráº­n
A = np.array([[4, 2],
              [1, 3]])

# TÃ­nh eigenvalues vÃ  eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)

print(f"Eigenvalues: {eigenvalues}")   # [5. 2.]
print(f"Eigenvectors:\n{eigenvectors}")

# Verify: Av = Î»v
for i in range(len(eigenvalues)):
    v = eigenvectors[:, i]
    lam = eigenvalues[i]
    
    Av = A @ v
    lam_v = lam * v
    
    print(f"Av = {Av}")
    print(f"Î»v = {lam_v}")
    print(f"Equal: {np.allclose(Av, lam_v)}")
```

**á»¨ng dá»¥ng**: 
- **PCA (Principal Component Analysis)**: TÃ¬m hÆ°á»›ng cÃ³ variance lá»›n nháº¥t
- **Spectral clustering**: PhÃ¢n cá»¥m dá»±a trÃªn eigenvalues
- **Google PageRank**: Eigenvector cá»§a ma tráº­n link
- **Stability analysis**: Kiá»ƒm tra há»‡ thá»‘ng Ä‘á»™ng

#### 1.4 SVD (Singular Value Decomposition)

**PhÃ¢n rÃ£ ma tráº­n**: 
```
A = UÎ£V^T
```
Trong Ä‘Ã³:
- A: Ma tráº­n gá»‘c (mÃ—n)
- U: Ma tráº­n trá»±c giao trÃ¡i (mÃ—m) - left singular vectors
- Î£: Ma tráº­n Ä‘Æ°á»ng chÃ©o (mÃ—n) - singular values
- V^T: Ma tráº­n trá»±c giao pháº£i (nÃ—n) - right singular vectors

**Code Python**:
```python
import numpy as np
from numpy.linalg import svd

# Ma tráº­n dá»¯ liá»‡u
A = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9],
              [10, 11, 12]])

print(f"Shape A: {A.shape}")  # (4, 3)

# SVD
U, S, VT = svd(A, full_matrices=False)

print(f"U shape: {U.shape}")    # (4, 3)
print(f"S shape: {S.shape}")    # (3,)
print(f"VT shape: {VT.shape}")  # (3, 3)

# Reconstruct A
S_matrix = np.diag(S)
A_reconstructed = U @ S_matrix @ VT
print(f"Reconstruction error: {np.linalg.norm(A - A_reconstructed)}")

# Low-rank approximation (compression)
k = 2  # Giá»¯ láº¡i 2 singular values lá»›n nháº¥t
A_compressed = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]
print(f"Compression ratio: {k * (U.shape[0] + VT.shape[1]) / A.size}")
```

**á»¨ng dá»¥ng thá»±c táº¿**:

1. **Image Compression**:
```python
from PIL import Image

# Load image
img = np.array(Image.open('image.jpg').convert('L'))
U, S, VT = svd(img, full_matrices=False)

# Compress vá»›i k singular values
k = 50
img_compressed = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# Save compressed image
Image.fromarray(img_compressed.astype(np.uint8)).save('compressed.jpg')
```

2. **Recommendation Systems (Collaborative Filtering)**:
```python
# User-Item rating matrix
ratings = np.array([
    [5, 3, 0, 1],
    [4, 0, 0, 1],
    [1, 1, 0, 5],
    [1, 0, 0, 4],
    [0, 1, 5, 4]
])

U, S, VT = svd(ratings, full_matrices=False)

# Low-rank approximation
k = 2
predicted_ratings = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# Predict missing ratings
print("Predicted ratings:")
print(predicted_ratings)
```

3. **Dimensionality Reduction**:
- Giá»‘ng PCA nhÆ°ng khÃ´ng cáº§n center data
- Giáº£m tá»« n dimensions xuá»‘ng k dimensions

#### 1.5 Norms vÃ  Distances
- **L1 norm** (Manhattan): |xâ‚| + |xâ‚‚| + ... + |xâ‚™|
- **L2 norm** (Euclidean): âˆš(xâ‚Â² + xâ‚‚Â² + ... + xâ‚™Â²)
- **á»¨ng dá»¥ng**: Regularization, Ä‘o khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u

---

## 2. Giáº£i tÃ­ch

### Táº§m quan trá»ng
Giáº£i tÃ­ch lÃ  ná»n táº£ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a cÃ¡c mÃ´ hÃ¬nh ML thÃ´ng qua gradient descent vÃ  backpropagation.

### CÃ¡c khÃ¡i niá»‡m chÃ­nh

#### 2.1 Äáº¡o hÃ m (Derivatives)
- **Äá»‹nh nghÄ©a**: Tá»‘c Ä‘á»™ thay Ä‘á»•i cá»§a hÃ m sá»‘
  ```
  f'(x) = lim[hâ†’0] (f(x+h) - f(x))/h
  ```
- **á»¨ng dá»¥ng**: TÃ¬m Ä‘á»™ dá»‘c Ä‘á»ƒ tá»‘i Æ°u hÃ³a hÃ m loss

#### 2.2 Äáº¡o hÃ m riÃªng (Partial Derivatives)
- **Äá»‹nh nghÄ©a**: Äáº¡o hÃ m theo má»™t biáº¿n khi giá»¯ cÃ¡c biáº¿n khÃ¡c cá»‘ Ä‘á»‹nh
  ```
  âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y
  ```
- **á»¨ng dá»¥ng**: TÃ­nh gradient trong khÃ´ng gian Ä‘a chiá»u

#### 2.3 Gradient
- **Vector gradient**: âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]
- **Ã nghÄ©a**: HÆ°á»›ng tÄƒng nhanh nháº¥t cá»§a hÃ m sá»‘
- **á»¨ng dá»¥ng**: Gradient Descent Algorithm

#### 2.4 Chain Rule (Quy táº¯c dÃ¢y chuyá»n)
- **CÃ´ng thá»©c**: (fâˆ˜g)'(x) = f'(g(x)) Â· g'(x)
- **á»¨ng dá»¥ng**: 
  - Backpropagation trong neural networks
  - TÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m há»£p

#### 2.5 Gradient Descent
```python
# Thuáº­t toÃ¡n cÆ¡ báº£n
Î¸ = Î¸ - Î± Â· âˆ‡J(Î¸)

# Î¸: tham sá»‘ cáº§n tá»‘i Æ°u
# Î±: learning rate
# âˆ‡J(Î¸): gradient cá»§a hÃ m loss
```

**CÃ¡c biáº¿n thá»ƒ**:
- **Batch Gradient Descent**: Sá»­ dá»¥ng toÃ n bá»™ dá»¯ liá»‡u
- **Stochastic Gradient Descent (SGD)**: Sá»­ dá»¥ng tá»«ng máº«u dá»¯ liá»‡u
- **Mini-batch Gradient Descent**: Sá»­ dá»¥ng batch nhá»

#### 2.6 Taylor Series
- **CÃ´ng thá»©c**: 
  ```
  f(x) = f(a) + f'(a)(x-a) + f''(a)(x-a)Â²/2! + ...
  ```
- **á»¨ng dá»¥ng**: Xáº¥p xá»‰ hÃ m sá»‘ phá»©c táº¡p

#### 2.7 Matrix Calculus

**Jacobian Matrix**:
Ma tráº­n Ä‘áº¡o hÃ m cá»§a vector function
```
f: â„â¿ â†’ â„áµ
J = [âˆ‚fâ‚/âˆ‚xâ‚  âˆ‚fâ‚/âˆ‚xâ‚‚  ...  âˆ‚fâ‚/âˆ‚xâ‚™]
    [âˆ‚fâ‚‚/âˆ‚xâ‚  âˆ‚fâ‚‚/âˆ‚xâ‚‚  ...  âˆ‚fâ‚‚/âˆ‚xâ‚™]
    [...      ...      ...  ...    ]
    [âˆ‚fâ‚˜/âˆ‚xâ‚  âˆ‚fâ‚˜/âˆ‚xâ‚‚  ...  âˆ‚fâ‚˜/âˆ‚xâ‚™]
```

**Code Python**:
```python
import numpy as np

def f(x):
    """Vector function f: RÂ² â†’ RÂ²"""
    return np.array([
        x[0]**2 + x[1]**2,
        x[0] * x[1]
    ])

def jacobian(x):
    """Jacobian cá»§a f táº¡i x"""
    return np.array([
        [2*x[0], 2*x[1]],
        [x[1], x[0]]
    ])

x = np.array([3.0, 4.0])
J = jacobian(x)
print(f"Jacobian táº¡i {x}:\n{J}")

# Numerical verification
h = 1e-7
J_numerical = np.zeros((2, 2))
for i in range(2):
    x_plus = x.copy()
    x_plus[i] += h
    J_numerical[:, i] = (f(x_plus) - f(x)) / h

print(f"Jacobian numerical:\n{J_numerical}")
```

**Hessian Matrix**:
Ma tráº­n Ä‘áº¡o hÃ m báº­c 2
```
f: â„â¿ â†’ â„
H = [âˆ‚Â²f/âˆ‚xâ‚Â²    âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚‚  ...  âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚™]
    [âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚‚Â²    ...  âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚™]
    [...         ...        ...  ...       ]
    [âˆ‚Â²f/âˆ‚xâ‚™âˆ‚xâ‚  âˆ‚Â²f/âˆ‚xâ‚™âˆ‚xâ‚‚  ...  âˆ‚Â²f/âˆ‚xâ‚™Â²  ]
```

**á»¨ng dá»¥ng Hessian**:
- **Newton's method**: Tá»‘i Æ°u báº­c 2
  ```
  x_{n+1} = x_n - Hâ»Â¹âˆ‡f(x_n)
  ```
- **Kiá»ƒm tra convexity**: Náº¿u H positive definite â†’ hÃ m convex
- **Second-order optimization**: L-BFGS

```python
def f(x):
    """f(x) = xâ‚Â² + 2xâ‚‚Â²"""
    return x[0]**2 + 2*x[1]**2

def hessian(x):
    """Hessian cá»§a f"""
    return np.array([
        [2, 0],
        [0, 4]
    ])

x = np.array([1.0, 1.0])
H = hessian(x)
print(f"Hessian:\n{H}")

# Check positive definite (convex)
eigenvalues = np.linalg.eigvals(H)
print(f"Eigenvalues: {eigenvalues}")
print(f"Positive definite: {all(eigenvalues > 0)}")
```

#### 2.8 Backpropagation Chi Tiáº¿t

**Neural Network Forward Pass**:
```
Layer 1: zâ‚ = Wâ‚x + bâ‚
         aâ‚ = Ïƒ(zâ‚)
Layer 2: zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚
         aâ‚‚ = Ïƒ(zâ‚‚)
Output:  Å· = aâ‚‚
Loss:    L = (y - Å·)Â²
```

**Backward Pass (Chain Rule)**:
```
âˆ‚L/âˆ‚Wâ‚‚ = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚Wâ‚‚
       = âˆ‚L/âˆ‚Å· Â· Ïƒ'(zâ‚‚) Â· aâ‚áµ€

âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚Å· Â· âˆ‚Å·/âˆ‚zâ‚‚ Â· âˆ‚zâ‚‚/âˆ‚aâ‚ Â· âˆ‚aâ‚/âˆ‚zâ‚ Â· âˆ‚zâ‚/âˆ‚Wâ‚
       = âˆ‚L/âˆ‚Å· Â· Ïƒ'(zâ‚‚) Â· Wâ‚‚áµ€ Â· Ïƒ'(zâ‚) Â· xáµ€
```

**Code Implementation**:
```python
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, z):
        s = self.sigmoid(z)
        return s * (1 - s)
    
    def forward(self, X):
        """Forward propagation"""
        self.z1 = X @ self.W1 + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = self.a1 @ self.W2 + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2
    
    def backward(self, X, y, learning_rate=0.01):
        """Backward propagation"""
        m = X.shape[0]
        
        # Output layer gradient
        dL_da2 = 2 * (self.a2 - y) / m
        dL_dz2 = dL_da2 * self.sigmoid_derivative(self.z2)
        
        # Hidden layer gradient
        dL_da1 = dL_dz2 @ self.W2.T
        dL_dz1 = dL_da1 * self.sigmoid_derivative(self.z1)
        
        # Weight gradients
        dL_dW2 = self.a1.T @ dL_dz2
        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)
        dL_dW1 = X.T @ dL_dz1
        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)
        
        # Update weights
        self.W2 -= learning_rate * dL_dW2
        self.b2 -= learning_rate * dL_db2
        self.W1 -= learning_rate * dL_dW1
        self.b1 -= learning_rate * dL_db1
    
    def train(self, X, y, epochs=1000):
        for epoch in range(epochs):
            # Forward
            y_pred = self.forward(X)
            
            # Loss
            loss = np.mean((y - y_pred)**2)
            
            # Backward
            self.backward(X, y)
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Example usage
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])  # XOR problem

nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
nn.train(X, y, epochs=5000)

# Test
predictions = nn.forward(X)
print(f"Predictions:\n{predictions}")
```

---

## 3. XÃ¡c suáº¥t vÃ  Thá»‘ng kÃª

### Táº§m quan trá»ng
XÃ¡c suáº¥t lÃ  cÆ¡ sá»Ÿ cá»§a machine learning, Ä‘áº·c biá»‡t trong xá»­ lÃ½ uncertainty vÃ  inference.

### CÃ¡c khÃ¡i niá»‡m chÃ­nh

#### 3.1 XÃ¡c suáº¥t cÆ¡ báº£n
- **XÃ¡c suáº¥t**: P(A) âˆˆ [0, 1]
- **XÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n**: P(A|B) = P(Aâˆ©B)/P(B)
- **Äá»‹nh lÃ½ Bayes**: 
  ```
  P(A|B) = P(B|A) Â· P(A) / P(B)
  ```

#### 3.2 Biáº¿n ngáº«u nhiÃªn
- **Rá»i ráº¡c**: Sá»‘ láº§n tung Ä‘á»“ng xu
- **LiÃªn tá»¥c**: Chiá»u cao, cÃ¢n náº·ng

#### 3.3 CÃ¡c phÃ¢n phá»‘i xÃ¡c suáº¥t quan trá»ng

**PhÃ¢n phá»‘i Bernoulli**:
- MÃ´ táº£ thÃ­ nghiá»‡m cÃ³ 2 káº¿t quáº£ (0 hoáº·c 1)
- P(X=1) = p, P(X=0) = 1-p

**PhÃ¢n phá»‘i Gaussian (Normal)**:
```
f(x) = (1/âˆš(2Ï€ÏƒÂ²)) Â· e^(-(x-Î¼)Â²/(2ÏƒÂ²))
```
- **á»¨ng dá»¥ng**: MÃ´ hÃ¬nh hÃ³a dá»¯ liá»‡u tá»± nhiÃªn, noise

**PhÃ¢n phá»‘i Multinomial**:
- Má»Ÿ rá»™ng cá»§a Bernoulli cho nhiá»u lá»›p
- **á»¨ng dá»¥ng**: Classification problems

**PhÃ¢n phá»‘i Poisson**:
```
P(X=k) = (Î»áµ Â· eâ»áµ) / k!
```
- **á»¨ng dá»¥ng**: Äáº¿m sá»‘ events trong khoáº£ng thá»i gian
- VÃ­ dá»¥: Sá»‘ email nháº­n trong 1 giá»

**PhÃ¢n phá»‘i Exponential**:
```
f(x) = Î»eâ»áµË£, x â‰¥ 0
```
- **á»¨ng dá»¥ng**: Thá»i gian chá» giá»¯a cÃ¡c events
- VÃ­ dá»¥: Thá»i gian giá»¯a 2 cuá»™c gá»i

**PhÃ¢n phá»‘i Beta**:
```
f(x) = x^(Î±-1)(1-x)^(Î²-1) / B(Î±,Î²)
```
- **á»¨ng dá»¥ng**: Prior distribution cho xÃ¡c suáº¥t trong Bayesian
- VÃ­ dá»¥: MÃ´ hÃ¬nh A/B testing

**Code Visualization**:
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Gaussian
x = np.linspace(-5, 5, 1000)
y_gaussian = stats.norm.pdf(x, loc=0, scale=1)

# Poisson
k = np.arange(0, 20)
y_poisson = stats.poisson.pmf(k, mu=5)

# Exponential
x_exp = np.linspace(0, 5, 1000)
y_exp = stats.expon.pdf(x_exp, scale=1)

# Beta
x_beta = np.linspace(0, 1, 1000)
y_beta = stats.beta.pdf(x_beta, a=2, b=5)

# Plot
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].plot(x, y_gaussian)
axes[0, 0].set_title('Gaussian Distribution')

axes[0, 1].bar(k, y_poisson)
axes[0, 1].set_title('Poisson Distribution')

axes[1, 0].plot(x_exp, y_exp)
axes[1, 0].set_title('Exponential Distribution')

axes[1, 1].plot(x_beta, y_beta)
axes[1, 1].set_title('Beta Distribution')

plt.tight_layout()
plt.show()
```

#### 3.4 CÃ¡c thá»‘ng kÃª mÃ´ táº£
- **Mean (Trung bÃ¬nh)**: Î¼ = E[X]
- **Variance (PhÆ°Æ¡ng sai)**: ÏƒÂ² = E[(X-Î¼)Â²]
- **Standard Deviation (Äá»™ lá»‡ch chuáº©n)**: Ïƒ = âˆšÏƒÂ²
- **Covariance**: Äo má»‘i quan há»‡ giá»¯a 2 biáº¿n
  ```
  Cov(X,Y) = E[(X-Î¼â‚“)(Y-Î¼áµ§)]
  ```

#### 3.5 Maximum Likelihood Estimation (MLE)
- **Má»¥c tiÃªu**: TÃ¬m tham sá»‘ Î¸ sao cho dá»¯ liá»‡u quan sÃ¡t Ä‘Æ°á»£c cÃ³ xÃ¡c suáº¥t cao nháº¥t
  ```
  Î¸Ì‚ = argmax L(Î¸|data)
  ```

#### 3.6 Bayes Networks
- **MÃ´ hÃ¬nh Ä‘á»“ thá»‹ xÃ¡c suáº¥t**: Biá»ƒu diá»…n má»‘i quan há»‡ cÃ³ Ä‘iá»u kiá»‡n
- **á»¨ng dá»¥ng**: 
  - Spam filtering
  - Diagnosis systems
  - Naive Bayes classifier

#### 3.7 Hypothesis Testing

**Quy trÃ¬nh kiá»ƒm Ä‘á»‹nh giáº£ thuyáº¿t**:
1. Äáº·t giáº£ thuyáº¿t null (Hâ‚€) vÃ  alternative (Hâ‚)
2. Chá»n má»©c significance (Î±, thÆ°á»ng lÃ  0.05)
3. TÃ­nh test statistic
4. TÃ­nh p-value
5. Káº¿t luáº­n: reject Hâ‚€ náº¿u p-value < Î±

**T-test**:
```python
from scipy import stats

# One-sample t-test
data = np.array([1.2, 2.3, 1.8, 2.5, 1.9])
population_mean = 2.0
t_stat, p_value = stats.ttest_1samp(data, population_mean)
print(f"T-statistic: {t_stat}, P-value: {p_value}")

# Two-sample t-test
group1 = np.array([1.2, 2.3, 1.8, 2.5, 1.9])
group2 = np.array([2.1, 2.8, 2.3, 3.0, 2.6])
t_stat, p_value = stats.ttest_ind(group1, group2)
print(f"T-statistic: {t_stat}, P-value: {p_value}")
```

**Chi-Square Test**:
```python
# Test independence cá»§a 2 categorical variables
observed = np.array([[10, 20, 30],
                     [6, 9, 17]])
chi2, p_value, dof, expected = stats.chi2_contingency(observed)
print(f"Chi-square: {chi2}, P-value: {p_value}")
```

#### 3.8 Confidence Intervals

**CÃ´ng thá»©c**:
```
CI = xÌ„ Â± z(Î±/2) Â· (Ïƒ/âˆšn)
```

**Code Python**:
```python
from scipy import stats
import numpy as np

data = np.array([1.2, 2.3, 1.8, 2.5, 1.9, 2.1, 2.0])

# TÃ­nh confidence interval 95%
confidence = 0.95
mean = np.mean(data)
std_err = stats.sem(data)
ci = stats.t.interval(confidence, len(data)-1, 
                      loc=mean, scale=std_err)

print(f"Mean: {mean:.3f}")
print(f"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]")
```

#### 3.9 Correlation vÃ  Causation

**Pearson Correlation**:
```
r = Cov(X,Y) / (Ïƒâ‚“ Â· Ïƒáµ§)
r âˆˆ [-1, 1]
```

**Code**:
```python
import numpy as np
from scipy.stats import pearsonr, spearmanr

x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])

# Pearson correlation
r_pearson, p_value = pearsonr(x, y)
print(f"Pearson r: {r_pearson:.3f}, p-value: {p_value:.3f}")

# Spearman correlation (rank-based)
r_spearman, p_value = spearmanr(x, y)
print(f"Spearman r: {r_spearman:.3f}, p-value: {p_value:.3f}")

# Visualization
import matplotlib.pyplot as plt

plt.scatter(x, y)
plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), 'r')
plt.title(f'Correlation: r = {r_pearson:.3f}')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

**LÆ°u Ã½ quan trá»ng**:
- **Correlation â‰  Causation**: Hai biáº¿n tÆ°Æ¡ng quan khÃ´ng cÃ³ nghÄ©a lÃ  má»™t biáº¿n gÃ¢y ra biáº¿n kia
- VÃ­ dá»¥: Ice cream sales vÃ  drowning deaths cÃ³ correlation cao (do cÃ¹ng tÄƒng vÃ o mÃ¹a hÃ¨)

---

## 4. Tá»‘i Æ°u hÃ³a

### Táº§m quan trá»ng
Tá»‘i Æ°u hÃ³a lÃ  cá»‘t lÃµi cá»§a viá»‡c training cÃ¡c mÃ´ hÃ¬nh ML - tÃ¬m tham sá»‘ tá»‘t nháº¥t Ä‘á»ƒ minimize loss.

### CÃ¡c khÃ¡i niá»‡m chÃ­nh

#### 4.1 Convex Optimization
- **HÃ m lá»“i**: f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y)
- **Æ¯u Ä‘iá»ƒm**: CÃ³ global minimum duy nháº¥t
- **á»¨ng dá»¥ng**: Linear regression, SVM

#### 4.2 Loss Functions

**Regression**:
- **MSE**: L = (1/n)Î£(yáµ¢ - Å·áµ¢)Â²
- **MAE**: L = (1/n)Î£|yáµ¢ - Å·áµ¢|

**Classification**:
- **Cross-Entropy**: L = -Î£yáµ¢log(Å·áµ¢)
- **Hinge Loss**: L = max(0, 1 - yÂ·Å·)

#### 4.3 Regularization
**L1 Regularization (Lasso)**:
```
J(Î¸) = Loss(Î¸) + Î»Î£|Î¸áµ¢|
```
- **Äáº·c Ä‘iá»ƒm**: Táº¡o sparsity (nhiá»u trá»ng sá»‘ = 0)

**L2 Regularization (Ridge)**:
```
J(Î¸) = Loss(Î¸) + Î»Î£Î¸áµ¢Â²
```
- **Äáº·c Ä‘iá»ƒm**: Giáº£m Ä‘á»™ lá»›n cá»§a trá»ng sá»‘

#### 4.4 Advanced Optimization Algorithms
- **Momentum**: TÄƒng tá»‘c trong hÆ°á»›ng nháº¥t quÃ¡n
- **Adam**: Adaptive learning rate cho tá»«ng tham sá»‘
- **RMSprop**: Äiá»u chá»‰nh learning rate dá»±a trÃªn gradient gáº§n Ä‘Ã¢y

#### 4.5 Constrained Optimization
- **Lagrange Multipliers**: Tá»‘i Æ°u vá»›i rÃ ng buá»™c
  ```
  L(x,Î») = f(x) + Î»g(x)
  ```
- **á»¨ng dá»¥ng**: SVM optimization

---

## 5. LÃ½ thuyáº¿t thÃ´ng tin

### Táº§m quan trá»ng
LÃ½ thuyáº¿t thÃ´ng tin cung cáº¥p framework Ä‘á»ƒ Ä‘o lÆ°á»ng uncertainty vÃ  information content.

### CÃ¡c khÃ¡i niá»‡m chÃ­nh

#### 5.1 Entropy
- **Shannon Entropy**: 
  ```
  H(X) = -Î£ P(x)logâ‚‚P(x)
  ```
- **Ã nghÄ©a**: Äo Ä‘á»™ báº¥t Ä‘á»‹nh/surprise trung bÃ¬nh
- **á»¨ng dá»¥ng**: Decision trees (information gain)

#### 5.2 Cross-Entropy
```
H(p,q) = -Î£ p(x)log q(x)
```
- **á»¨ng dá»¥ng**: Loss function trong classification

#### 5.3 KL-Divergence
```
D_KL(P||Q) = Î£ P(x)log(P(x)/Q(x))
```
- **Ã nghÄ©a**: Äo khoáº£ng cÃ¡ch giá»¯a 2 phÃ¢n phá»‘i
- **á»¨ng dá»¥ng**: 
  - Variational autoencoders (VAE)
  - Model evaluation

#### 5.4 Mutual Information
```
I(X;Y) = H(X) - H(X|Y)
```
- **Ã nghÄ©a**: LÆ°á»£ng thÃ´ng tin chung giá»¯a 2 biáº¿n
- **á»¨ng dá»¥ng**: Feature selection

---

## 6. Lá»™ trÃ¬nh há»c táº­p

### Giai Ä‘oáº¡n 1: Ná»n táº£ng (2-3 thÃ¡ng)
1. **Äáº¡i sá»‘ tuyáº¿n tÃ­nh cÆ¡ báº£n**
   - Vector, ma tráº­n, phÃ©p toÃ¡n cÆ¡ báº£n
   - Eigenvalues, eigenvectors
   - Thá»±c hÃ nh vá»›i NumPy

2. **Giáº£i tÃ­ch**
   - Äáº¡o hÃ m, Ä‘áº¡o hÃ m riÃªng
   - Gradient
   - Chain rule

3. **XÃ¡c suáº¥t cÆ¡ báº£n**
   - XÃ¡c suáº¥t, xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n
   - Äá»‹nh lÃ½ Bayes
   - CÃ¡c phÃ¢n phá»‘i cÆ¡ báº£n

### Giai Ä‘oáº¡n 2: á»¨ng dá»¥ng (3-4 thÃ¡ng)
1. **Tá»‘i Æ°u hÃ³a**
   - Gradient descent vÃ  cÃ¡c biáº¿n thá»ƒ
   - Loss functions
   - Regularization

2. **Thá»‘ng kÃª nÃ¢ng cao**
   - MLE
   - Hypothesis testing
   - Bayesian inference

3. **Thá»±c hÃ nh**
   - Implement cÃ¡c thuáº­t toÃ¡n tá»« Ä‘áº§u
   - Linear regression
   - Logistic regression

### Giai Ä‘oáº¡n 3: NÃ¢ng cao (3-4 thÃ¡ng)
1. **Deep Learning Math**
   - Backpropagation chi tiáº¿t
   - Optimization algorithms (Adam, RMSprop)
   - Batch normalization

2. **Advanced Topics**
   - Information theory
   - Convex optimization
   - Matrix calculus

---

## 7. TÃ i nguyÃªn há»c táº­p

### SÃ¡ch giÃ¡o khoa
1. **"Mathematics for Machine Learning"** - Marc Peter Deisenroth
   - Free PDF: https://mml-book.github.io/
   - Bao quÃ¡t toÃ n diá»‡n cÃ¡c chá»§ Ä‘á»

2. **"Deep Learning"** - Ian Goodfellow
   - Chapters 2-4 vá» math foundations
   - Link: https://www.deeplearningbook.org/

3. **"Pattern Recognition and Machine Learning"** - Christopher Bishop
   - XÃ¡c suáº¥t vÃ  thá»‘ng kÃª cho ML

### KhÃ³a há»c online
1. **Khan Academy**
   - Linear Algebra
   - Calculus
   - Statistics

2. **3Blue1Brown (YouTube)**
   - Essence of Linear Algebra
   - Essence of Calculus
   - Visualization tuyá»‡t vá»i

3. **Coursera**
   - Mathematics for Machine Learning Specialization
   - Imperial College London

### CÃ´ng cá»¥ thá»±c hÃ nh
```python
# NumPy - Äáº¡i sá»‘ tuyáº¿n tÃ­nh
import numpy as np

# Ma tráº­n
A = np.array([[1, 2], [3, 4]])
# Eigenvalues
eigenvalues, eigenvectors = np.linalg.eig(A)

# SciPy - Tá»‘i Æ°u hÃ³a
from scipy.optimize import minimize

# Matplotlib - Visualization
import matplotlib.pyplot as plt

# TensorFlow/PyTorch - Deep Learning
import tensorflow as tf
import torch
```

### Websites há»¯u Ã­ch
1. **Distill.pub** - Giáº£i thÃ­ch ML vá»›i visualization
2. **colah.github.io** - Blog vá» neural networks
3. **Towards Data Science** - Tutorials vÃ  articles

---

## ğŸ“ BÃ i táº­p thá»±c hÃ nh

### Äáº¡i sá»‘ tuyáº¿n tÃ­nh
1. Implement matrix multiplication tá»« Ä‘áº§u
2. Viáº¿t thuáº­t toÃ¡n PCA
3. TÃ­nh eigenvalues vÃ  eigenvectors

### Giáº£i tÃ­ch
1. Implement gradient descent cho linear regression
2. TÃ­nh gradient cá»§a neural network Ä‘Æ¡n giáº£n
3. Visualize gradient descent trÃªn cÃ¡c hÃ m khÃ¡c nhau

### XÃ¡c suáº¥t
1. Implement Naive Bayes classifier
2. TÃ­nh posterior probability vá»›i Bayes' theorem
3. Visualize cÃ¡c phÃ¢n phá»‘i xÃ¡c suáº¥t

### Tá»‘i Æ°u hÃ³a
1. So sÃ¡nh SGD vs Mini-batch GD vs Batch GD
2. Implement Adam optimizer
3. ThÃªm L1/L2 regularization vÃ o model

---

## ğŸ¯ Checklist kiáº¿n thá»©c

### Äáº¡i sá»‘ tuyáº¿n tÃ­nh
- [ ] Hiá»ƒu vector vÃ  ma tráº­n
- [ ] ThÃ nh tháº¡o cÃ¡c phÃ©p toÃ¡n ma tráº­n
- [ ] Eigenvalues vÃ  eigenvectors
- [ ] SVD
- [ ] PCA

### Giáº£i tÃ­ch
- [ ] Äáº¡o hÃ m vÃ  Ä‘áº¡o hÃ m riÃªng
- [ ] Gradient vÃ  chain rule
- [ ] Gradient descent
- [ ] Backpropagation

### XÃ¡c suáº¥t
- [ ] XÃ¡c suáº¥t cÆ¡ báº£n vÃ  cÃ³ Ä‘iá»u kiá»‡n
- [ ] Äá»‹nh lÃ½ Bayes
- [ ] CÃ¡c phÃ¢n phá»‘i xÃ¡c suáº¥t
- [ ] MLE
- [ ] Bayesian inference

### Tá»‘i Æ°u hÃ³a
- [ ] Loss functions
- [ ] Regularization
- [ ] Optimization algorithms
- [ ] Convex optimization

---

## ğŸ’¡ Tips há»c táº­p

1. **Há»c lÃ½ thuyáº¿t káº¿t há»£p thá»±c hÃ nh**
   - Äá»«ng chá»‰ Ä‘á»c cÃ´ng thá»©c
   - Implement tá»« Ä‘áº§u vá»›i Python

2. **Visualization**
   - Váº½ Ä‘á»“ thá»‹ Ä‘á»ƒ hiá»ƒu concepts
   - Sá»­ dá»¥ng Matplotlib, Plotly

3. **LÃ m projects**
   - Ãp dá»¥ng toÃ¡n vÃ o bÃ i toÃ¡n thá»±c táº¿
   - Kaggle competitions

4. **Há»c theo nhÃ³m**
   - Giáº£i thÃ­ch cho ngÆ°á»i khÃ¡c
   - Tháº£o luáº­n cÃ¡c váº¥n Ä‘á» khÃ³

5. **KiÃªn nháº«n**
   - ToÃ¡n há»c cáº§n thá»i gian
   - Ã”n táº­p thÆ°á»ng xuyÃªn

---

## ğŸ“š Káº¿t luáº­n

ToÃ¡n há»c lÃ  cÃ´ng cá»¥ khÃ´ng thá»ƒ thiáº¿u trong AI/ML. KhÃ´ng cáº§n pháº£i master táº¥t cáº£ trÆ°á»›c khi báº¯t Ä‘áº§u, nhÆ°ng nÃªn hiá»ƒu sÃ¢u cÃ¡c concepts cÆ¡ báº£n:

1. **Äáº¡i sá»‘ tuyáº¿n tÃ­nh** - Xá»­ lÃ½ dá»¯ liá»‡u Ä‘a chiá»u
2. **Giáº£i tÃ­ch** - Tá»‘i Æ°u hÃ³a models
3. **XÃ¡c suáº¥t** - Xá»­ lÃ½ uncertainty
4. **Tá»‘i Æ°u hÃ³a** - Training algorithms

HÃ£y há»c song song giá»¯a lÃ½ thuyáº¿t vÃ  thá»±c hÃ nh, vÃ  Ä‘á»«ng ngáº¡i implement cÃ¡c thuáº­t toÃ¡n tá»« Ä‘áº§u Ä‘á»ƒ hiá»ƒu sÃ¢u bÃªn trong!

---

**Táº¡o bá»Ÿi**: GitHub Copilot  
**NgÃ y**: 30/01/2026  
**PhiÃªn báº£n**: 1.0
